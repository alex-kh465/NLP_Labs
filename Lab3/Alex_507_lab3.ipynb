{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download required nltk data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves synonyms, nouns, verbs, adjectives, adverbs, and the definition of the third meaning of a given word using WordNet.\n",
    "def get_synonyms(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) >= 3:\n",
    "        third_meaning = synsets[2]  # Get 3rd meaning\n",
    "        synonyms = third_meaning.lemma_names()\n",
    "        nouns = [lemma for syn in synsets for lemma in syn.lemma_names() if syn.pos() == 'n']\n",
    "        verbs = [lemma for syn in synsets for lemma in syn.lemma_names() if syn.pos() == 'v']\n",
    "        adjectives = [lemma for syn in synsets for lemma in syn.lemma_names() if syn.pos() == 'a']\n",
    "        adverbs = [lemma for syn in synsets for lemma in syn.lemma_names() if syn.pos() == 'r']\n",
    "        definition = third_meaning.definition()\n",
    "        return synonyms, nouns, verbs, adjectives, adverbs, definition\n",
    "    return [], [], [], [], [], \"No third meaning available\"\n",
    "\n",
    "def get_antonyms(word):\n",
    "    antonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "    return antonyms\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def compare_stemming_lemmatization(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return {\n",
    "        \"Lemma\": lemmatizer.lemmatize(word),\n",
    "        \"Porter\": porter.stem(word),\n",
    "        \"Lancaster\": lancaster.stem(word),\n",
    "        \"Snowball\": snowball.stem(word)\n",
    "    }\n",
    "\n",
    "def pos_tagging(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(words)\n",
    "\n",
    "def named_entity_recognition(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    return nltk.chunk.ne_chunk(pos_tags)\n",
    "\n",
    "def dependency_and_constituency_parsing(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Dependency Parsing\n",
    "    dependency_parsing = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "    # Constituency Parsing - Instead of doc.sents, use list(doc.sents)\n",
    "    constituency_parsing = [sent.text for sent in doc.sents]  # Extracting sentences properly\n",
    "\n",
    "    return dependency_parsing, constituency_parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: bear\n",
      "Synonyms: ['bear']\n",
      "Nouns: ['bear', 'bear']\n",
      "Verbs: ['bear', 'give_birth', 'deliver', 'bear', 'birth', 'have', 'digest', 'endure', 'stick_out', 'stomach', 'bear', 'stand', 'tolerate', 'support', 'brook', 'abide', 'suffer', 'put_up', 'bear', 'bear', 'turn_out', 'bear', 'take_over', 'accept', 'assume', 'hold', 'bear', 'carry', 'contain', 'yield', 'pay', 'bear', 'wear', 'bear', 'behave', 'acquit', 'bear', 'deport', 'conduct', 'comport', 'carry', 'bear', 'hold', 'hold', 'carry', 'bear', 'have_a_bun_in_the_oven', 'bear', 'carry', 'gestate', 'expect']\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "Definition: have\n",
      "Antonyms: ['bull']\n",
      "Lemmatization: bear\n",
      "Stemming vs Lemmatization: {'Lemma': 'bear', 'Porter': 'bear', 'Lancaster': 'bear', 'Snowball': 'bear'}\n",
      "\n",
      "Word: set\n",
      "Synonyms: ['set', 'exercise_set']\n",
      "Nouns: ['set', 'set', 'set', 'exercise_set', 'stage_set', 'set', 'set', 'circle', 'band', 'lot', 'bent', 'set', 'set', 'set', 'hardening', 'solidifying', 'solidification', 'set', 'curing', 'Set', 'Seth', 'set', 'set', 'readiness', 'set']\n",
      "Verbs: ['put', 'set', 'place', 'pose', 'position', 'lay', 'determine', 'set', 'specify', 'set', 'determine', 'define', 'fix', 'limit', 'set', 'mark', 'set', 'set', 'fix', 'prepare', 'set_up', 'ready', 'gear_up', 'set', 'set', 'set', 'localize', 'localise', 'place', 'set', 'go_down', 'go_under', 'arrange', 'set', 'plant', 'set', 'set', 'jell', 'set', 'congeal', 'typeset', 'set', 'set', 'set', 'countersink', 'set', 'sic', 'set', 'place', 'put', 'set', 'rig', 'set', 'set_up', 'set_up', 'lay_out', 'set', 'adjust', 'set', 'correct', 'fructify', 'set', 'dress', 'arrange', 'set', 'do', 'coif', 'coiffe', 'coiffure']\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "Definition: several exercises intended to be done in series\n",
      "Antonyms: ['rise']\n",
      "Lemmatization: set\n",
      "Stemming vs Lemmatization: {'Lemma': 'set', 'Porter': 'set', 'Lancaster': 'set', 'Snowball': 'set'}\n",
      "\n",
      "Word: square\n",
      "Synonyms: ['public_square', 'square']\n",
      "Nouns: ['square', 'foursquare', 'square', 'second_power', 'public_square', 'square', 'square', 'square', 'lame', 'square', 'square_toes', 'square', 'square']\n",
      "Verbs: ['square', 'square_up', 'square', 'square', 'square', 'square', 'square', 'feather', 'square', 'feather', 'square']\n",
      "Adjectives: ['square', 'straight', 'square']\n",
      "Adverbs: ['squarely', 'square', 'squarely', 'square', 'squarely', 'square']\n",
      "Definition: an open area at the meeting of two or more streets\n",
      "Antonyms: ['round', 'crooked']\n",
      "Lemmatization: square\n",
      "Stemming vs Lemmatization: {'Lemma': 'square', 'Porter': 'squar', 'Lancaster': 'squ', 'Snowball': 'squar'}\n",
      "\n",
      "Word: lead\n",
      "Synonyms: ['lead', 'track', 'trail']\n",
      "Nouns: ['lead', 'lead', 'Pb', 'atomic_number_82', 'lead', 'track', 'trail', 'lead', 'lead', 'lead', 'lead-in', 'lede', 'lead', 'star', 'principal', 'lead', 'lead', 'tip', 'lead', 'steer', 'confidential_information', 'wind', 'hint', 'lead', 'lead_story', 'spark_advance', 'lead', 'leash', 'tether', 'lead', 'lead', 'leading', 'lead', 'pencil_lead', 'jumper_cable', 'jumper_lead', 'lead', 'booster_cable', 'lead']\n",
      "Verbs: ['lead', 'take', 'direct', 'conduct', 'guide', 'leave', 'result', 'lead', 'lead', 'lead', 'head', 'lead', 'run', 'go', 'pass', 'lead', 'extend', 'head', 'lead', 'lead', 'top', 'contribute', 'lead', 'conduce', 'conduct', 'lead', 'direct', 'go', 'lead', 'precede', 'lead', 'run', 'lead', 'moderate', 'chair', 'lead']\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "Definition: evidence pointing to a possible solution\n",
      "Antonyms: ['deficit', 'follow']\n",
      "Lemmatization: lead\n",
      "Stemming vs Lemmatization: {'Lemma': 'lead', 'Porter': 'lead', 'Lancaster': 'lead', 'Snowball': 'lead'}\n",
      "\n",
      "Word: criteria\n",
      "Synonyms: []\n",
      "Nouns: []\n",
      "Verbs: []\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "Definition: No third meaning available\n",
      "Antonyms: []\n",
      "Lemmatization: criterion\n",
      "Stemming vs Lemmatization: {'Lemma': 'criterion', 'Porter': 'criteria', 'Lancaster': 'criter', 'Snowball': 'criteria'}\n",
      "\n",
      "POS Tagging: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "\n",
      "Named Entity Recognition: (S\n",
      "  The/DT\n",
      "  quick/JJ\n",
      "  brown/NN\n",
      "  fox/NN\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  lazy/JJ\n",
      "  dog/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word_list = [\"bear\", \"set\", \"square\", \"lead\", \"criteria\"]\n",
    "for word in word_list:\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    synonyms, nouns, verbs, adjectives, adverbs, definition = get_synonyms(word)\n",
    "    print(f\"Synonyms: {synonyms}\")\n",
    "    print(f\"Nouns: {nouns}\")\n",
    "    print(f\"Verbs: {verbs}\")\n",
    "    print(f\"Adjectives: {adjectives}\")\n",
    "    print(f\"Adverbs: {adverbs}\")\n",
    "    print(f\"Definition: {definition}\")\n",
    "    print(f\"Antonyms: {get_antonyms(word)}\")\n",
    "    print(f\"Lemmatization: {lemmatize_word(word)}\")\n",
    "    print(f\"Stemming vs Lemmatization: {compare_stemming_lemmatization(word)}\")\n",
    "\n",
    "# PoS tagging and NER example\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(\"\\nPOS Tagging:\", pos_tagging(sentence))\n",
    "print(\"\\nNamed Entity Recognition:\", named_entity_recognition(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parsing: [('The', 'det', 'fox'), ('quick', 'amod', 'fox'), ('brown', 'amod', 'fox'), ('fox', 'nsubj', 'jumps'), ('jumps', 'ROOT', 'jumps'), ('over', 'prep', 'jumps'), ('the', 'det', 'dog'), ('lazy', 'amod', 'dog'), ('dog', 'pobj', 'over'), ('.', 'punct', 'jumps')]\n",
      "\n",
      "Constituency Parsing: ['The quick brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "# # Dependency and Constituency Parsing\n",
    "dep_parsing, const_parsing = dependency_and_constituency_parsing(sentence)\n",
    "print(\"\\nDependency Parsing:\", dep_parsing)\n",
    "print(\"\\nConstituency Parsing:\", const_parsing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
