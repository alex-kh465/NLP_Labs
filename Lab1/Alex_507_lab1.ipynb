{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample paragraph\n",
    "text = '''I'm really diving into the world of AR/ VR development 🎮🕶️, and honestly, \n",
    "it's such an exciting space! I don't think there’s anything quite like the immersion it offers, right? \n",
    "You get to experience things firsthand—whether it's exploring new environments or interacting with digital elements like they're part of the real world. \n",
    "But hey, it’s not all smooth sailing,\n",
    "😅 there are challenges to overcome, like optimizing performance and ensuring seamless user experience.\n",
    "Still, it’s hard to ignore the potential this technology has for transforming industries. So, I'm all in—let’s see what the future holds! 🌟'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aspire_Lays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading neccessary files\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR/VR', 'development', '🎮🕶️', ',', 'and', 'honestly', ',', 'it', \"'s\", 'such', 'an', 'exciting', 'space', '!', 'I', 'do', \"n't\", 'think', 'there', '’', 's', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', ',', 'right', '?', 'You', 'get', 'to', 'experience', 'things', 'firsthand—whether', 'it', \"'s\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', \"'re\", 'part', 'of', 'the', 'real', 'world', '.', 'But', 'hey', ',', 'it', '’', 's', 'not', 'all', 'smooth', 'sailing', ',', '😅', 'there', 'are', 'challenges', 'to', 'overcome', ',', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', '.', 'Still', ',', 'it', '’', 's', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', '.', 'So', ',', 'I', \"'m\", 'all', 'in—let', '’', 's', 'see', 'what', 'the', 'future', 'holds', '!', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#word tokenize will tokenize based on words as it suggests,same goes for sentence tokenizer as well\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "words_token = []\n",
    "words_token = word_tokenize(text)\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm really diving into the world of AR/VR development 🎮🕶️, and honestly, \\nit's such an exciting space!\", \"I don't think there’s anything quite like the immersion it offers, right?\", \"You get to experience things firsthand—whether it's exploring new environments or interacting with digital elements like they're part of the real world.\", 'But hey, it’s not all smooth sailing,\\n😅 there are challenges to overcome, like optimizing performance and ensuring seamless user experience.', 'Still, it’s hard to ignore the potential this technology has for transforming industries.', \"So, I'm all in—let’s see what the future holds!\", '🌟']\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "sentence_token = sent_tokenize(text)\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'm', 'really', 'diving', 'into', 'the', 'world', 'of', 'AR', 'VR', 'development', 'and', 'honestly', 'it', 's', 'such', 'an', 'exciting', 'space', 'I', 'don', 't', 'think', 'there', 's', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', 'right', 'You', 'get', 'to', 'experience', 'things', 'firsthand', 'whether', 'it', 's', 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', 're', 'part', 'of', 'the', 'real', 'world', 'But', 'hey', 'it', 's', 'not', 'all', 'smooth', 'sailing', 'there', 'are', 'challenges', 'to', 'overcome', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', 'Still', 'it', 's', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', 'So', 'I', 'm', 'all', 'in', 'let', 's', 'see', 'what', 'the', 'future', 'holds']\n"
     ]
    }
   ],
   "source": [
    "#punctuation based tokenizer, tokenizes by punctuations, can be implemented using regular expression\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Tokenizer to split by words, excluding punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR/VR', 'development', '🎮🕶️', ',', 'and', 'honestly', ',', 'it', \"'s\", 'such', 'an', 'exciting', 'space', '!', 'I', 'do', \"n't\", 'think', 'there’s', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', ',', 'right', '?', 'You', 'get', 'to', 'experience', 'things', 'firsthand—whether', 'it', \"'s\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', \"'re\", 'part', 'of', 'the', 'real', 'world.', 'But', 'hey', ',', 'it’s', 'not', 'all', 'smooth', 'sailing', ',', '😅', 'there', 'are', 'challenges', 'to', 'overcome', ',', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience.', 'Still', ',', 'it’s', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries.', 'So', ',', 'I', \"'m\", 'all', 'in—let’s', 'see', 'what', 'the', 'future', 'holds', '!', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#The TreebankWordTokenizer uses rules from the Penn Treebank for splitting text. It handles contractions, punctuation, and hyphenated words better than basic tokenizers.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tree_bank_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tree_bank_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR', '/', 'VR', 'development', '🎮', '🕶', '️', ',', 'and', 'honestly', ',', \"it's\", 'such', 'an', 'exciting', 'space', '!', 'I', \"don't\", 'think', 'there', '’', 's', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', ',', 'right', '?', 'You', 'get', 'to', 'experience', 'things', 'firsthand', '—', 'whether', \"it's\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', \"they're\", 'part', 'of', 'the', 'real', 'world', '.', 'But', 'hey', ',', 'it', '’', 's', 'not', 'all', 'smooth', 'sailing', ',', '😅', 'there', 'are', 'challenges', 'to', 'overcome', ',', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', '.', 'Still', ',', 'it', '’', 's', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', '.', 'So', ',', \"I'm\", 'all', 'in', '—', 'let', '’', 's', 'see', 'what', 'the', 'future', 'holds', '!', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#The TweetTokenizer is designed for social media content, handling hashtags, mentions, emoticons, and other informal text features.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR/', 'VR', 'development', '🎮🕶️,', 'and', 'honestly,', \"it's\", 'such', 'an', 'exciting', 'space!', 'I', \"don't\", 'think', 'there’s', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers,', 'right?', 'You', 'get', 'to', 'experience', 'things', 'firsthand—whether', \"it's\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', \"they're\", 'part', 'of', 'the', 'real', 'world.', 'But', 'hey,', 'it’s', 'not', 'all', 'smooth', 'sailing,', '😅', 'there', 'are', 'challenges', 'to', 'overcome,', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience.', 'Still,', 'it’s', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries.', 'So,', \"I'm\", 'all', 'in—let’s', 'see', 'what', 'the', 'future', 'holds!', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#The MWETokenizer is used to treat multi-word expressions (like New York or machine learning) as a single token.\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer([('AR/ ', 'VR')])\n",
    "tokens = tokenizer.tokenize(text.split())\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR/VR', 'development', '🎮🕶️', 'and', 'honestly', 'it', \"'s\", 'such', 'an', 'exciting', 'space', 'I', 'do', \"n't\", 'think', 'there', '’', 's', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', 'right', 'You', 'get', 'to', 'experience', 'things', 'firsthand—whether', 'it', \"'s\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', \"'re\", 'part', 'of', 'the', 'real', 'world', 'But', 'hey', 'it', '’', 's', 'not', 'all', 'smooth', 'sailing', '😅', 'there', 'are', 'challenges', 'to', 'overcome', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', 'Still', 'it', '’', 's', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', 'So', 'I', \"'m\", 'all', 'in—let', '’', 's', 'see', 'what', 'the', 'future', 'holds', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#TextBlob provides an easy-to-use API for NLP tasks, including tokenization.\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)\n",
    "tokens = blob.words\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'really', 'diving', 'into', 'the', 'world', 'of', 'AR', '/', 'VR', 'development', '🎮', '🕶', '️', ',', 'and', 'honestly', ',', '\\n', 'it', \"'s\", 'such', 'an', 'exciting', 'space', '!', 'I', 'do', \"n't\", 'think', 'there', '’s', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', ',', 'right', '?', '\\n', 'You', 'get', 'to', 'experience', 'things', 'firsthand', '—', 'whether', 'it', \"'s\", 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', \"'re\", 'part', 'of', 'the', 'real', 'world', '.', '\\n', 'But', 'hey', ',', 'it', '’s', 'not', 'all', 'smooth', 'sailing', ',', '\\n', '😅', 'there', 'are', 'challenges', 'to', 'overcome', ',', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', '.', '\\n', 'Still', ',', 'it', '’s', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', '.', 'So', ',', 'I', \"'m\", 'all', 'in', '—', 'let', '’s', 'see', 'what', 'the', 'future', 'holds', '!', '🌟']\n"
     ]
    }
   ],
   "source": [
    "#spaCy is a powerful NLP library. Its tokenizer handles complex text processing like punctuation, contractions, and more.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'diving', 'into', 'the', 'world', 'of', 'ar', 'vr', 'development', 'and', 'honestly', 'it', 'such', 'an', 'exciting', 'space', 'don', 'think', 'there', 'anything', 'quite', 'like', 'the', 'immersion', 'it', 'offers', 'right', 'you', 'get', 'to', 'experience', 'things', 'firsthand', 'whether', 'it', 'exploring', 'new', 'environments', 'or', 'interacting', 'with', 'digital', 'elements', 'like', 'they', 're', 'part', 'of', 'the', 'real', 'world', 'but', 'hey', 'it', 'not', 'all', 'smooth', 'sailing', 'there', 'are', 'challenges', 'to', 'overcome', 'like', 'optimizing', 'performance', 'and', 'ensuring', 'seamless', 'user', 'experience', 'still', 'it', 'hard', 'to', 'ignore', 'the', 'potential', 'this', 'technology', 'has', 'for', 'transforming', 'industries', 'so', 'all', 'in', 'let', 'see', 'what', 'the', 'future', 'holds']\n"
     ]
    }
   ],
   "source": [
    "#Gensim offers simple preprocessing utilities for tokenization, often used for topic modeling or word embeddings.\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [19], [12], [], [5], [1], [10], [8], [8], [18], [], [13], [3], [21], [3], [4], [11], [], [3], [4], [2], [6], [], [2], [9], [1], [], [20], [6], [5], [8], [13], [], [6], [15], [], [10], [5], [], [21], [5], [], [13], [1], [21], [1], [8], [6], [16], [12], [1], [4], [2], [], [26], [27], [28], [], [], [10], [4], [13], [], [9], [6], [4], [1], [7], [2], [8], [18], [], [], [], [3], [2], [19], [7], [], [7], [17], [14], [9], [], [10], [4], [], [1], [22], [14], [3], [2], [3], [4], [11], [], [7], [16], [10], [14], [1], [], [], [3], [], [13], [6], [4], [19], [2], [], [2], [9], [3], [4], [23], [], [2], [9], [1], [5], [1], [24], [7], [], [10], [4], [18], [2], [9], [3], [4], [11], [], [29], [17], [3], [2], [1], [], [8], [3], [23], [1], [], [2], [9], [1], [], [3], [12], [12], [1], [5], [7], [3], [6], [4], [], [3], [2], [], [6], [15], [15], [1], [5], [7], [], [], [5], [3], [11], [9], [2], [], [], [], [18], [6], [17], [], [11], [1], [2], [], [2], [6], [], [1], [22], [16], [1], [5], [3], [1], [4], [14], [1], [], [2], [9], [3], [4], [11], [7], [], [15], [3], [5], [7], [2], [9], [10], [4], [13], [25], [20], [9], [1], [2], [9], [1], [5], [], [3], [2], [19], [7], [], [1], [22], [16], [8], [6], [5], [3], [4], [11], [], [4], [1], [20], [], [1], [4], [21], [3], [5], [6], [4], [12], [1], [4], [2], [7], [], [6], [5], [], [3], [4], [2], [1], [5], [10], [14], [2], [3], [4], [11], [], [20], [3], [2], [9], [], [13], [3], [11], [3], [2], [10], [8], [], [1], [8], [1], [12], [1], [4], [2], [7], [], [8], [3], [23], [1], [], [2], [9], [1], [18], [19], [5], [1], [], [16], [10], [5], [2], [], [6], [15], [], [2], [9], [1], [], [5], [1], [10], [8], [], [20], [6], [5], [8], [13], [], [], [], [30], [17], [2], [], [9], [1], [18], [], [], [3], [2], [24], [7], [], [4], [6], [2], [], [10], [8], [8], [], [7], [12], [6], [6], [2], [9], [], [7], [10], [3], [8], [3], [4], [11], [], [], [31], [], [2], [9], [1], [5], [1], [], [10], [5], [1], [], [14], [9], [10], [8], [8], [1], [4], [11], [1], [7], [], [2], [6], [], [6], [21], [1], [5], [14], [6], [12], [1], [], [], [8], [3], [23], [1], [], [6], [16], [2], [3], [12], [3], [32], [3], [4], [11], [], [16], [1], [5], [15], [6], [5], [12], [10], [4], [14], [1], [], [10], [4], [13], [], [1], [4], [7], [17], [5], [3], [4], [11], [], [7], [1], [10], [12], [8], [1], [7], [7], [], [17], [7], [1], [5], [], [1], [22], [16], [1], [5], [3], [1], [4], [14], [1], [], [], [7], [2], [3], [8], [8], [], [], [3], [2], [24], [7], [], [9], [10], [5], [13], [], [2], [6], [], [3], [11], [4], [6], [5], [1], [], [2], [9], [1], [], [16], [6], [2], [1], [4], [2], [3], [10], [8], [], [2], [9], [3], [7], [], [2], [1], [14], [9], [4], [6], [8], [6], [11], [18], [], [9], [10], [7], [], [15], [6], [5], [], [2], [5], [10], [4], [7], [15], [6], [5], [12], [3], [4], [11], [], [3], [4], [13], [17], [7], [2], [5], [3], [1], [7], [], [], [7], [6], [], [], [3], [19], [12], [], [10], [8], [8], [], [3], [4], [25], [8], [1], [2], [24], [7], [], [7], [1], [1], [], [20], [9], [10], [2], [], [2], [9], [1], [], [15], [17], [2], [17], [5], [1], [], [9], [6], [8], [13], [7], [], [], [33]]\n",
      "{'e': 1, 't': 2, 'i': 3, 'n': 4, 'r': 5, 'o': 6, 's': 7, 'l': 8, 'h': 9, 'a': 10, 'g': 11, 'm': 12, 'd': 13, 'c': 14, 'f': 15, 'p': 16, 'u': 17, 'y': 18, \"'\": 19, 'w': 20, 'v': 21, 'x': 22, 'k': 23, '’': 24, '—': 25, '🎮': 26, '🕶': 27, '️': 28, 'q': 29, 'b': 30, '😅': 31, 'z': 32, '🌟': 33}\n"
     ]
    }
   ],
   "source": [
    "#Keras provides a Tokenizer class primarily used for preparing text data for deep learning models.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(text)\n",
    "tokens = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "print(tokens)\n",
    "print(tokenizer.word_index)  # To see the word-to-index mapping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
