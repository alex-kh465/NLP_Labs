{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample sentences for the task\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day long.\",\n",
    "    \"A fox is a wild animal and it loves to jump.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer with different parameters and fit the model\n",
    "count_vectorizer_1 = CountVectorizer(max_df=1, min_df=1, max_features=3)\n",
    "count_vectorizer_2 = CountVectorizer(max_df=2, min_df=1, max_features=3)\n",
    "count_vectorizer_3 = CountVectorizer(max_df=0.75, min_df=1, max_features=3)\n",
    "\n",
    "# Fit and transform the data\n",
    "count_matrix_1 = count_vectorizer_1.fit_transform(sentences)\n",
    "count_matrix_2 = count_vectorizer_2.fit_transform(sentences)\n",
    "count_matrix_3 = count_vectorizer_3.fit_transform(sentences)\n",
    "\n",
    "# Display the feature names and their corresponding count vectors for each case\n",
    "print(\"CountVectorizer with max_df=1, min_df=1, max_features=3:\")\n",
    "print(pd.DataFrame(count_matrix_1.toarray(), columns=count_vectorizer_1.get_feature_names_out()))\n",
    "\n",
    "print(\"\\nCountVectorizer with max_df=2, min_df=1, max_features=3:\")\n",
    "print(pd.DataFrame(count_matrix_2.toarray(), columns=count_vectorizer_2.get_feature_names_out()))\n",
    "\n",
    "print(\"\\nCountVectorizer with max_df=0.75, min_df=1, max_features=3:\")\n",
    "print(pd.DataFrame(count_matrix_3.toarray(), columns=count_vectorizer_3.get_feature_names_out()))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Display the TF-IDF features\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Transformer is an exceptional innovation in the field of Deep Learning, contributed by Ashish \n",
    "Vaswani et al. (2017), Google. The transformer is the most influential Neural Network model that has \n",
    "shown outstanding performance on various NLP tasks including Machine Reading Comprehension, \n",
    "Machine translation and sentence classification. Attention mechanism and parallelization are the \n",
    "prominent features in the transformers. Consequently, it can facilitate long-range dependencies \n",
    "without any gradient vanishing or gradient explosion problems and it overcomes the drawbacks of \n",
    "the existing methods such as RNN and LSTM. The transformer is executed with an encoder-decoder \n",
    "mechanism and the original article of transformers # “Attention All You Need”.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a. Word and Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Punctuation Removal\n",
    "filtered_words_no_punctuation = [word for word in filtered_words if word not in string.punctuation]\n",
    "filtered_words_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Frequency Distribution\n",
    "fdist = FreqDist(filtered_words_no_punctuation)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Frequency Distribution\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# e. Stemming and Lemmatization\n",
    "# Stemming (using PorterStemmer and LancasterStemmer)\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter_stems = [porter_stemmer.stem(word) for word in filtered_words_no_punctuation]\n",
    "lancaster_stems = [lancaster_stemmer.stem(word) for word in filtered_words_no_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words_no_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output Stems and Lemmas\n",
    "print(\"\\nPorter Stems:\", porter_stems[:10])\n",
    "print(\"\\nLancaster Stems:\", lancaster_stems[:10])\n",
    "print(\"\\nLemmatized Words:\", lemmatized_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# g. Named Entity Recognition (NER)\n",
    "# Load Spacy's pre-trained model for NER\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "ner = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "print(\"\\nNamed Entities:\", ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. PoS Tagging\n",
    "pos_tags = pos_tag(filtered_words_no_punctuation)\n",
    "print(\"\\nPart-of-Speech Tags:\", pos_tags[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
