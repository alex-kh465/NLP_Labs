{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Load English stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Given Corpus\n",
    "corpus = \"\"\"Transformer is an exceptional innovation in the field of Deep Learning, contributed by Ashish \n",
    "Vaswani et al. (2017), Google. The transformer is the most influential Neural Network model that has \n",
    "shown outstanding performance on various NLP tasks including Machine Reading Comprehension, \n",
    "Machine translation and sentence classification. Attention mechanism and parallelization are the \n",
    "prominent features in the transformers. Consequently, it can facilitate long-range dependencies \n",
    "without any gradient vanishing or gradient explosion problems and it overcomes the drawbacks of \n",
    "the existing methods such as RNN and LSTM. The transformer is executed with an encoder-decoder \n",
    "mechanism and the original article of transformers # “Attention All You Need”.\"\"\"\n",
    "\n",
    "# **(a) Word and Sentence Tokenization**\n",
    "sent_tokens = sent_tokenize(corpus)\n",
    "word_tokens = word_tokenize(corpus)\n",
    "\n",
    "print(\"Sentence Tokenization:\\n\", sent_tokens)\n",
    "print(\"\\nWord Tokenization:\\n\", word_tokens)\n",
    "\n",
    "# **(b) Stopwords Removal**\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"\\nWords after Stopword Removal:\\n\", filtered_words)\n",
    "\n",
    "# **(c) Punctuation Removal**\n",
    "filtered_words = [word for word in filtered_words if word not in string.punctuation]\n",
    "\n",
    "print(\"\\nWords after Punctuation Removal:\\n\", filtered_words)\n",
    "\n",
    "# **(d) Frequency Distribution and Visualization**\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Visualizing using WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Frequency Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# **(e) Stemming (Porter and Lancaster) and Lemmatization**\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_words_porter = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "stemmed_words_lancaster = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\nPorter Stemmer Output:\\n\", stemmed_words_porter)\n",
    "print(\"\\nLancaster Stemmer Output:\\n\", stemmed_words_lancaster)\n",
    "print(\"\\nLemmatization Output:\\n\", lemmatized_words)\n",
    "\n",
    "# **(f) Part-of-Speech (PoS) Tagging**\n",
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"\\nPoS Tagging:\\n\", pos_tags)\n",
    "\n",
    "# **(g) Named Entity Recognition (NER)**\n",
    "doc = nlp(corpus)\n",
    "print(\"\\nNamed Entities:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample Text\n",
    "text = \"\"\"Mickey Mouse, a cheerful and optimistic mouse clad in red shorts and yellow shoes, \n",
    "is the iconic mascot of The Walt Disney Company. Debuting in 1928, this spunky character has \n",
    "charmed audiences for generations with his adventures and can-do attitude.\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "nltk.download('punkt')\n",
    "sentences = [word_tokenize(text.lower())]  # Convert to lowercase and tokenize\n",
    "\n",
    "# Train Word2Vec Model (Skip-gram)\n",
    "skipgram_model = Word2Vec(sentences, vector_size=50, window=5, sg=1, min_count=1)\n",
    "\n",
    "# Train Word2Vec Model (CBOW)\n",
    "cbow_model = Word2Vec(sentences, vector_size=50, window=5, sg=0, min_count=1)\n",
    "\n",
    "print(\"Skip-gram Example:\", skipgram_model.wv.most_similar('mouse'))\n",
    "print(\"CBOW Example:\", cbow_model.wv.most_similar('mouse'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(model, title):\n",
    "    words = list(model.wv.key_to_index.keys())[:10]  # Get first 10 words\n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Reduce dimensions using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_vectors = pca.fit_transform(word_vectors)\n",
    "\n",
    "    # Plot the vectors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=reduced_vectors[:, 0], y=reduced_vectors[:, 1], marker='o')\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        plt.text(reduced_vectors[i, 0], reduced_vectors[i, 1], word, fontsize=12)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot Word Embeddings for Skip-gram and CBOW\n",
    "plot_embeddings(skipgram_model, \"Word2Vec - Skip-gram Visualization\")\n",
    "plot_embeddings(cbow_model, \"Word2Vec - CBOW Visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample Sentences\n",
    "sentences = [\n",
    "    \"Mickey Mouse is a famous Disney character.\",\n",
    "    \"Walt Disney created Mickey Mouse in 1928.\",\n",
    "    \"The Disney Company is known for Mickey Mouse and animated films.\"\n",
    "]\n",
    "\n",
    "# Count Vectorizer with different parameters\n",
    "vectorizer1 = CountVectorizer(max_df=1)\n",
    "vectorizer2 = CountVectorizer(max_df=2)\n",
    "vectorizer3 = CountVectorizer(max_df=0.75, min_df=1, max_features=3)\n",
    "\n",
    "# Fit the vectorizer on sentences\n",
    "X1 = vectorizer1.fit_transform(sentences).toarray()\n",
    "X2 = vectorizer2.fit_transform(sentences).toarray()\n",
    "X3 = vectorizer3.fit_transform(sentences).toarray()\n",
    "\n",
    "print(\"\\nCount Vectorizer (max_df=1):\\n\", vectorizer1.get_feature_names_out())\n",
    "print(X1)\n",
    "\n",
    "print(\"\\nCount Vectorizer (max_df=2):\\n\", vectorizer2.get_feature_names_out())\n",
    "print(X2)\n",
    "\n",
    "print(\"\\nCount Vectorizer (max_df=0.75, min_df=1, max_features=3):\\n\", vectorizer3.get_feature_names_out())\n",
    "print(X3)\n",
    "\n",
    "# **TF-IDF Implementation**\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(sentences).toarray()\n",
    "\n",
    "print(\"\\nTF-IDF Feature Names:\\n\", tfidf.get_feature_names_out())\n",
    "print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
